{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1zXeeDYFliHq7dWQyUcLw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huseyincavusbi/gguf_works/blob/main/llama_cpp_python_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL3iSsOaTfO5",
        "outputId": "354eda3e-8746-4186-8969-0be3c2b5cede"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9TZ7wf7TS9j",
        "outputId": "30d469d0-1ccb-4ffb-d5c5-85fc0c6da052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp311-cp311-linux_x86_64.whl size=4237788 sha256=7c3baac181fd9b34c1a3d4628f0b7e15f76270bfb242c4ced3d20f79ea244903\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/b6/cf/7315ec7b0149210d2d4447d9c3338b36d10e56a1ecddcd35c0\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.14\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/gemma-3N-finetune.Q8_0.gguf\"\n",
        "\n",
        "\n",
        "llm = Llama(\n",
        "      model_path=model_path,\n",
        "      n_gpu_layers=0,  # Ensure CPU usage\n",
        "      verbose=False    # Suppresses the logs\n",
        ")\n",
        "\n",
        "# Define your prompt\n",
        "prompt = \"Who are you?\"\n",
        "\n",
        "# Run the inference\n",
        "output = llm(prompt, max_tokens=50)\n",
        "\n",
        "# Print only the result\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flw8JKTJhUMc",
        "outputId": "95b62ad1-7f20-4530-b2ee-9925d9f356c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind.  I am an open-weights model, which means I'm widely available to the public. I take text and image as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuous Q&A with Streaming and Stats\n",
        "import time\n",
        "\n",
        "MAX_TOKENS_PER_TURN = 512\n",
        "\n",
        "# This is the \"memory\" of the conversation.\n",
        "conversation_history = \"You are a helpful AI assistant. You will answer the questions of the user.\\n\"\n",
        "\n",
        "print(\"Chat session started!\")\n",
        "print(\"Mode: Streaming (Robust). Type 'quit', 'exit', or 'q' to end.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # 1. Get user input\n",
        "        user_input = input(\"You: \") # User sees this structure\n",
        "\n",
        "        # 2. Check for an exit command\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"\\nExiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # 3. Construct the prompt with history\n",
        "        prompt = f\"{conversation_history}Human: {user_input}\\nAssistant:\" # Model sees this structure\n",
        "\n",
        "        # We tokenize the prompt to get its length. We must encode it to utf-8.\n",
        "        prompt_tokens = len(llm.tokenize(prompt.encode('utf-8')))\n",
        "\n",
        "        # --- Start Timer ---\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 4. Call the model with stream=True\n",
        "        stream = llm(\n",
        "            prompt=prompt,\n",
        "            max_tokens=MAX_TOKENS_PER_TURN,\n",
        "            stop=[\"Human:\"], # See 3. Construct the prompt with history section\n",
        "            stream=True,\n",
        "            echo=False\n",
        "        )\n",
        "\n",
        "        # 5. Loop through the stream and print the output\n",
        "        print(\"Assistant: \", end=\"\")\n",
        "        full_response = \"\"\n",
        "        completion_tokens = 0\n",
        "        for chunk in stream:\n",
        "            text_chunk = chunk['choices'][0]['text']\n",
        "            print(text_chunk, end=\"\", flush=True)\n",
        "            full_response += text_chunk\n",
        "            completion_tokens += 1 # Count generated tokens\n",
        "\n",
        "        # --- End Timer ---\n",
        "        end_time = time.time()\n",
        "        print() # Add a newline\n",
        "\n",
        "        # 6. Calculate and Display Stats\n",
        "        duration = end_time - start_time\n",
        "        if duration > 0:\n",
        "            tokens_per_second = completion_tokens / duration\n",
        "        else:\n",
        "            tokens_per_second = float('inf')\n",
        "\n",
        "        print(\n",
        "            f\"[Stats: \"\n",
        "            f\"Prompt: {prompt_tokens} tokens, \" # Using our pre-calculated value\n",
        "            f\"Completion: {completion_tokens} tokens, \" # Using our counter\n",
        "            f\"Speed: {tokens_per_second:.2f} tokens/s]\\n\"\n",
        "            f\"--------------------------------------------------\"\n",
        "        )\n",
        "\n",
        "        # 7. Update the conversation history\n",
        "        conversation_history += f\"Human: {user_input}\\nAssistant: {full_response.strip()}\\n\"\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nChat interrupted by user. Exiting.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL3DT514leif",
        "outputId": "d9efd7d4-f229-4f3c-8577-4eaa797c4f70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat session started!\n",
            "Mode: Streaming (Robust). Type 'quit', 'exit', or 'q' to end.\n",
            "--------------------------------------------------\n",
            "You: whats your name?\n",
            "Assistant:  I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind. \n",
            "\n",
            "\n",
            "[Stats: Prompt: 27 tokens, Completion: 27 tokens, Speed: 2.07 tokens/s]\n",
            "--------------------------------------------------\n",
            "You: cool what unique capabilities do you have?\n",
            "Assistant:  As an open-weights AI assistant, I have several unique capabilities. I am proficient in understanding and generating human-like text. I can also process images as inputs and use that information to inform my text-based responses. I'm designed to be helpful and informative, and I am openly available to the public.\n",
            "\n",
            "[Stats: Prompt: 64 tokens, Completion: 66 tokens, Speed: 2.31 tokens/s]\n",
            "--------------------------------------------------\n",
            "You: q\n",
            "\n",
            "Exiting chat. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}